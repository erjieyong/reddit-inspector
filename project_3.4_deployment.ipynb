{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdced7f-4496-4502-863e-9dcd3a307985",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "To deploy all the models under 1 common docker file and upload to google cloud. Visualise using streamlit.\n",
    "\n",
    "There are total 4 models for deployment\n",
    "- Classification between Science and Philosophy subreddit `project_3.31_main.ipynb`\n",
    "- Multiclassification of flairs (reddit's name for sub-categories) `project_3.32_multiclass.ipynb`\n",
    "- Auto summariser of text `project_3.33_summary.ipynb`\n",
    "- Sentiment Analysis of text `project_3.34_sentiment.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c5d1-3c63-4f2c-8168-c0a68a5e3776",
   "metadata": {},
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13b70c04-6ab1-4245-a4be-d67bdd82dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py \n",
    "from flask import Flask, request \n",
    "import pandas as pd \n",
    "import os \n",
    "import joblib\n",
    "import time\n",
    "import mlflow.pyfunc\n",
    "# We have to import the following io, otherwise pandas will output error because we only passing in a string and not a dictionary.\n",
    "# https://stackoverflow.com/questions/63553845/pandas-read-json-valueerror-protocol-not-known/63655099#63655099\n",
    "from io import StringIO\n",
    "# To scrap website and run summariser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "# To perform sentiment analysis\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "\n",
    "api = Flask('ModelEndpoint')\n",
    "\n",
    "#########################\n",
    "##    Classification   ##\n",
    "#########################\n",
    "model_classify = joblib.load(\"./models/model_classify.pkl\")\n",
    "\n",
    "#########################\n",
    "## MultiClassification ##\n",
    "#########################\n",
    "model_multiclass = mlflow.pyfunc.load_model(model_uri=\"./models/model_multiclass\")\n",
    "transform_multiclass = joblib.load(\"./models/model_multiclass/tfidf.pkl\")\n",
    "#function to run multiclass predictions\n",
    "def multiclass_predict(text):\n",
    "    flair_dict = {1: 'Medicine',\n",
    "                  2: 'Social Science',\n",
    "                  3: 'Animal Science',\n",
    "                  4: 'Anthropology',\n",
    "                  5: 'Environment',\n",
    "                  6: 'Psychology',\n",
    "                  7: 'Health',\n",
    "                  8: 'Nanoscience',\n",
    "                  9: 'Engineering',\n",
    "                  10: 'Biology',\n",
    "                  11: 'Earth Science',\n",
    "                  12: 'Astronomy',\n",
    "                  13: 'Genetics',\n",
    "                  14: 'Economics',\n",
    "                  15: 'Paleontology',\n",
    "                  16: 'Chemistry',\n",
    "                  17: 'Neuroscience',\n",
    "                  18: 'Cancer',\n",
    "                  19: 'Mathematics',\n",
    "                  20: 'Epidemiology',\n",
    "                  21: 'Physics',\n",
    "                  22: 'Geology',\n",
    "                  23: 'Materials Science',\n",
    "                  24: 'Computer Science',\n",
    "                  25: 'Breaking News',\n",
    "                  26: 'Retraction',\n",
    "                  27: 'Best of r/science'}\n",
    "    flair_no = model_multiclass.predict(transform_multiclass.transform(text))[0]\n",
    "    return flair_dict[flair_no]\n",
    "\n",
    "#########################\n",
    "##     Summariser      ##\n",
    "#########################\n",
    "#function to run summariser\n",
    "hf_summarizer = pipeline('summarization', 'sshleifer/distilbart-cnn-12-6')\n",
    "def summariser(url):\n",
    "    if len(url) > 1:\n",
    "        # pass in header in attempt to hide that this is an automated web crawler\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            all_p = soup.find_all('p')\n",
    "            relevant_p = ''\n",
    "            for p in all_p:\n",
    "                # We assume that if the sentence has less than 10 words, they are likely to be ads or link's descriptions to other pages. We are trying to keep this as general as possible so that we can scrap more sites without issues\n",
    "                if len(p.text.split(' ')) > 10:\n",
    "                    relevant_p += (p.text + ' ')\n",
    "            #we cap it to 700 because there's a limit of words that the hugging face model can take\n",
    "            relevant_p_trimmed = ' '.join(relevant_p.split(' ')[:700])\n",
    "            return hf_summarizer(relevant_p_trimmed)[0]['summary_text'], relevant_p\n",
    "        else:\n",
    "            #return error message if unable to crawl website\n",
    "            return \"ERROR! Unable to crawl website. Please check if the link is valid or if the website allows automated web crawling\", \"a\"\n",
    "    else:\n",
    "        #return error message if no the url length is only 1\n",
    "        return \"ERROR! Please pass in a valid url\", \"a\"\n",
    "\n",
    "#########################\n",
    "##      Sentiment      ##\n",
    "#########################\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "#function to retrieve sentiment and subjectivity\n",
    "def sentiment_predict(title, selftext, page):\n",
    "    text = title + selftext + page\n",
    "    spacy_output = nlp(text)\n",
    "    if spacy_output._.polarity > 0.33:\n",
    "        sentiment = 'Positive'\n",
    "    elif spacy_output._.polarity < -0.33:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    if spacy_output._.subjectivity < 0.5:\n",
    "        subjectivity = 'Objective'\n",
    "    else:\n",
    "        subjectivity = 'Subjective'\n",
    "    return sentiment, round(spacy_output._.polarity,2), subjectivity, round(spacy_output._.subjectivity,2)\n",
    "\n",
    "#########################\n",
    "##      FLASK API      ##\n",
    "#########################\n",
    "@api.route('/') \n",
    "def home(): \n",
    "    return {\"message\": \"Hello!\", \"success\": True}, 200\n",
    "\n",
    "@api.route('/predict', methods = ['POST']) \n",
    "def make_predictions():\n",
    "    user_input = request.get_json(force=True)\n",
    "    df_schema = {'title':str, 'selftext': str, 'url':str}\n",
    "    user_input_df = pd.read_json(StringIO(user_input), lines=True, dtype=df_schema)\n",
    "    combined_user_input = pd.Series(user_input_df['title'] + user_input_df['selftext'] + user_input_df['url']) #this will output as {'0':'all the texts'}\n",
    "    \n",
    "\n",
    "    #failsafe in case someone input nothing in either of the 3 fields\n",
    "    if len(user_input_df['title'].tolist()[0])==0:\n",
    "        #input 'a' as a common letter that should give no meaning in nlp\n",
    "        user_input_df['title'] = 'a'\n",
    "    if len(user_input_df['selftext'].tolist()[0])==0:\n",
    "        #input 'a' as a common letter that should give no meaning in nlp\n",
    "        user_input_df['selftext'] = 'a'\n",
    "    if len(user_input_df['url'].tolist()[0])==0:\n",
    "        #input 'a' as a common letter that should give no meaning in nlp\n",
    "        user_input_df['url'] = 'a'\n",
    "    \n",
    "    # return {'test':user_input_df['title'].tolist()[0]}\n",
    "    # PREDICTIONS\n",
    "    predict_class = model_classify.predict(combined_user_input).tolist() #we need to pass number to list so that it can be convered to json later   \n",
    "    predict_flair = multiclass_predict(combined_user_input) #For text, we don't need to pass in tolist\n",
    "    summary, page = summariser(user_input_df['url'].tolist()[0])\n",
    "    sentiment, senti_score, subjectivity, subj_score = sentiment_predict(user_input_df['title'].tolist()[0], user_input_df['selftext'].tolist()[0], page)\n",
    "    \n",
    "\n",
    "    # RETURN OUTPUT\n",
    "    if predict_class[0]==1:\n",
    "        return {'subreddit': 'Science', 'flair': predict_flair, 'summary':summary, 'sentiment': sentiment, 'senti_score':senti_score, 'subjectivity':subjectivity, 'subj_score':subj_score}\n",
    "    else:\n",
    "        return {'subreddit': 'Philosophy', 'summary':summary, 'sentiment': sentiment, 'senti_score':senti_score, 'subjectivity':subjectivity, 'subj_score':subj_score}\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    api.run(host='0.0.0.0', \n",
    "            debug=True, \n",
    "            port=int(os.environ.get(\"PORT\", 8080))\n",
    "           ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af8045-918c-4150-835d-b3c361fc7665",
   "metadata": {},
   "source": [
    "### Testing on localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fc64468-a53f-405b-b869-a7dbf9315385",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_science = {\"title\": \"Physicist Stephen Hawking dies aged 76\", \"selftext\": \"We regret to hear that Stephen Hawking died tonight at the age of 76. We are creating a megathread for discussion of this topic here. The typical r/science comment rules will not apply and we will allow mature, open discussion. This post may be updated as we are able. A few relevant links: Stephen Hawking's AMA on /r/science. BBC's Obituary for Stephen Hawking. If you would like to make a donation in his memory, the Stephen Hawking Foundation has the Dignity Campaign to help buy adapted wheelchair equipment for people suffering from motor neuron diseases. You could also consider donating to the ALS Association to support research into finding a cure for ALS and to provide support to ALS patients\", \"url\": \"http://www.bbc.com/news/uk-43396008\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68a3a532-3862-424d-8eec-e6ac9f63c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_philo = {\"title\": \"Only fragments of ancient Greek philosopher Epicurus’s writings remain. Among them are his Principal Doctrines: 40 brilliant, authoritative aphorisms that summarize the Epicurean approach to living a good life — an approach focused on removing pain & anxiety, & on emphasizing friendship & community.\", \"selftext\":\"\", \"url\": \"https://philosophybreak.com/articles/epicurus-principal-doctrines-40-aphorisms-for-living-well/?utm_source=reddit&utm_medium=social\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7caa9cda-7894-4138-b8db-8c94c6ce4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = {\"title\":\"A qualitative study of an incel discussion board says that incels justify their misogyny by seeing themselves as victims of women.\",\"selftext\":\"\",\"url\":\"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee021741-7861-4448-b0d3-a0c9ac001862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flair': 'Social Science', 'senti_score': 0.0, 'sentiment': 'Neutral', 'subj_score': 0.0, 'subjectivity': 'Objective', 'subreddit': 'Science', 'summary': 'ERROR! Please pass in a valid url'}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "api_url = 'http://localhost:8080' # specify the URL to access\n",
    "api_route = '/predict' # specify the `route` to access in the URL\n",
    "\n",
    "# we'll need to use `requests.post()` based on our earlier specification in `\\predict` route to only accept a `POST` request \n",
    "response = requests.post(f'{api_url}{api_route}', json=json.dumps(empty))\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101ba73-13dc-45ed-9037-542ff2e0b4d7",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36aec472-2da5-41f0-b94c-34cf666a868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# Use the official lightweight Python image from\n",
    "# https://hub.docker.com/_/python\n",
    "FROM python:3.8-slim \n",
    "\n",
    "# Copy all the files needed for the app to work\n",
    "COPY inference.py .\n",
    "COPY models/ ./models/\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install all the necessary libraries\n",
    "RUN pip install -r requirements.txt\n",
    "RUN python -m textblob.download_corpora\n",
    "RUN python -m spacy download en_core_web_sm\n",
    "\n",
    "# Run the API!\n",
    "CMD python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d7eb9b-3cd1-460c-b47b-a46aaa36c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "flask\n",
    "mlflow-skinny\n",
    "scikit-learn==0.23.2\n",
    "torch==1.10.2\n",
    "spacy\n",
    "spacytextblob\n",
    "bs4\n",
    "transformers\n",
    "lxml==4.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6fb03-26c1-4ee5-b466-2ca900db3047",
   "metadata": {},
   "source": [
    "## Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda130df-f5c0-464c-b4ae-5c5111afadf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Title of the page\n",
    "st.title(\"Science 🧪 vs 🧠 Philosophy Subreddit\")\n",
    "\n",
    "st.caption(\"\"\"Don't know which subreddit to share your posts? Use our app!\\n\n",
    "For testing purposes, the cells have been populated with the *most upvoted* post on Science subreddit. Change it to your own!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9addfc37-cddb-49c1-809a-e046cd846d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a streamlit_app.py\n",
    "\n",
    "#we have to put the inputs all inside a form to prevent the whole app from being re-run each time a input is change. \n",
    "# https://blog.streamlit.io/introducing-submit-button-and-forms/\n",
    "# Get user inputs\n",
    "with st.form(key='my_form'):\n",
    "    title = st.text_input('Post Title', 'Physicist Stephen Hawking dies aged 76')\n",
    "    selftext = st.text_area('Post Content', \"We regret to hear that Stephen Hawking died tonight at the age of 76. We are creating a megathread for discussion of this topic here. The typical r/science comment rules will not apply and we will allow mature, open discussion. This post may be updated as we are able. A few relevant links: Stephen Hawking's AMA on /r/science. BBC's Obituary for Stephen Hawking. If you would like to make a donation in his memory, the Stephen Hawking Foundation has the Dignity Campaign to help buy adapted wheelchair equipment for people suffering from motor neuron diseases. You could also consider donating to the ALS Association to support research into finding a cure for ALS and to provide support to ALS patients.\",\n",
    "                            height = 150)\n",
    "    url = st.text_input('URL', 'http://www.bbc.com/news/uk-43396008')\n",
    "    submit = st.form_submit_button(label='Inspect')\n",
    "    \n",
    "    user_input = {'title': title,\n",
    "                  'selftext': selftext,\n",
    "                  'url': url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8671d9c9-c892-46f3-bad5-410fb263fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a streamlit_app.py\n",
    "\n",
    "# code to run after submit button is pressed\n",
    "if submit:\n",
    "    with st.spinner('🪄 ✨Gathering magic dusts...✨'):\n",
    "        user_input = {'title': title,\n",
    "                      'selftext': selftext,\n",
    "                      'url': url}\n",
    "        # Code to post the user inputs to the API and get the predictions\n",
    "        # Paste the URL to your GCP Cloud Run API here!\n",
    "        api_url = 'https://science-philo-reddit-class-vrfckmfjmq-as.a.run.app'\n",
    "        api_route = '/predict'\n",
    "\n",
    "        response = requests.post(f'{api_url}{api_route}', json=json.dumps(user_input)) # json.dumps() converts dict to JSON\n",
    "        predictions = response.json()\n",
    "        \n",
    "        #SNOW!!\n",
    "        st.snow()\n",
    "        \n",
    "        st.header(\"Inspection Result\")\n",
    "\n",
    "        col11, col12 = st.columns(2)\n",
    "        col11.metric(\"Subreddit\", predictions['subreddit'], help = \"Most likely subreddit based on binary classification\")\n",
    "        if predictions['subreddit'] == 'Science':\n",
    "            col12.metric(\"Flair\", predictions['flair'], help = \"Most likely subcategory based on multiclass classification\")\n",
    "        st.caption(\"\"\"\n",
    "        <p style=\"color: grey;font-size: 80%;\">\n",
    "        The above section predicts the most accurate subreddit to post the article based on machine learning of 50,000 previous posts. For Science subreddit, further subcategory (flair) is suggested for you based on 25,000 previous classification\\n\n",
    "        \\n\n",
    "        </p>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        #hide the arrow which is default by streamlit\n",
    "        st.write(\n",
    "            \"\"\"\n",
    "            <style>\n",
    "            [data-testid=\"stMetricDelta\"] svg {\n",
    "                display: none;\n",
    "            }\n",
    "            </style>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True,\n",
    "        )\n",
    "        col21, col22 = st.columns(2)\n",
    "        col21.metric(\"Sentiment\", predictions['sentiment'], delta = predictions['senti_score'], delta_color = 'off', help = \"\"\"\n",
    "                     0.33 to 1 : Positive\\n\n",
    "                     -0.33 to 0.33 : Neutral\\n\n",
    "                     -1 to -0.33 : Negative\n",
    "                     \"\"\")\n",
    "        col22.metric(\"Subjectivity\", predictions['subjectivity'], delta = predictions['subj_score'], delta_color = 'off', help = \"\"\"\n",
    "                     0.5 to 1 : Subjective\\n\n",
    "                     0 to 0.5 : Objective\n",
    "                     \"\"\")\n",
    "        st.caption(\"\"\"\n",
    "        <p style=\"color: grey;font-size: 80%;\">\n",
    "        The above section is the cumulative sentiment of post title, post content and article after crawling through the url. A more neutral sentiment with objective tone is a better showcase that your posts are not informed by bias\\n\n",
    "        \\n\n",
    "        </p>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "        st.caption('<p class = \"myclass\">Article Summary 📃</p>', unsafe_allow_html=True)\n",
    "        st.markdown(predictions['summary'])\n",
    "        st.caption(\"\"\"\n",
    "        <p style=\"color: grey;font-size: 80%;\">\n",
    "        The above section is an auto generated summary of the article content after crawling through the url. 99.9% of subreddits does not have any post content other than title. Use our auto summary to fill up the gap!\\n\n",
    "        </p>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        st.markdown(\n",
    "             f\"\"\"\n",
    "             <style>\n",
    "             .css-rvekum .myclass {{\n",
    "                 color:black\n",
    "             }}\n",
    "             .css-dg4u6x .myclass {{\n",
    "                 color:white\n",
    "             }}\n",
    "             </style>\n",
    "             \"\"\",\n",
    "             unsafe_allow_html=True\n",
    "         )\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
